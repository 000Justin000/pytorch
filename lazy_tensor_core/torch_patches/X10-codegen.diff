diff --git a/aten/src/ATen/templates/aten_xla_type_default.cpp b/aten/src/ATen/templates/aten_xla_type_default.cpp
index 040a752156..fdc5ad8f26 100644
--- a/aten/src/ATen/templates/aten_xla_type_default.cpp
+++ b/aten/src/ATen/templates/aten_xla_type_default.cpp
@@ -1,16 +1,16 @@
 // ${generated_comment}
-#include <torch_xla/csrc/aten_xla_type_default.h>
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type_default.h>
 
 #include <ATen/Context.h>
 #include <torch/library.h>
 #include <ATen/CPUGeneratorImpl.h>
 
-#include <tensorflow/compiler/xla/xla_client/debug_macros.h>
-#include <tensorflow/compiler/xla/xla_client/metrics.h>
-#include <tensorflow/compiler/xla/xla_client/tf_logging.h>
-#include <torch_xla/csrc/aten_xla_bridge.h>
-#include <torch_xla/csrc/XLANativeFunctions.h>
-#include <torch_xla/csrc/function_call_tracker.h>
+#include <lazy_tensors/computation_client/debug_macros.h>
+#include <lazy_tensors/computation_client/metrics.h>
+#include <lazy_tensors/computation_client/ltc_logging.h>
+#include <lazy_tensor_core/csrc/aten_ltc_bridge.h>
+#include <lazy_tensor_core/csrc/ts_backend/XLANativeFunctions.h>
+#include <lazy_tensor_core/csrc/function_call_tracker.h>
 
 namespace ${cpp_namespace} {
 
diff --git a/tools/codegen/dest/gen_external_aten_fallbacks.py b/tools/codegen/dest/gen_external_aten_fallbacks.py
index 62fdd800b3..f61f4d3128 100644
--- a/tools/codegen/dest/gen_external_aten_fallbacks.py
+++ b/tools/codegen/dest/gen_external_aten_fallbacks.py
@@ -30,40 +30,7 @@ _FN_DENYLIST_REGEX = [
 # Instead, the codegen will figure out which ops to generate _out wrappers for
 # entirely from the yaml. Maintaining the same behavior as current XLA codegen for now.
 _FN_OUT = [
-    'abs',
     'add',
-    'acos',
-    'acosh',
-    'asin',
-    'asinh',
-    'atan',
-    'atan2',
-    'atanh',
-    'baddbmm',
-    'bernoulli',
-    'binary_cross_entropy',
-    'binary_cross_entropy_backward',
-    'clamp',
-    'div',
-    'gather',
-    'ger',
-    'hardsigmoid',
-    'kthvalue',
-    'index_select',
-    'inverse',
-    'log',
-    'masked_select',
-    'maximum',
-    'minimum',
-    'pow',
-    'prod',
-    'nonzero',
-    'round',
-    'normal',
-    'std',
-    'take',
-    'topk',
-    'var',
 ]
 
 # See Note [Auto generated composite kernels]
@@ -266,9 +233,9 @@ class GenExternalAtenFallback:
 
             return f"""\
 {dispatcher_sig.defn(name=func_name)} {{
-  XLA_FN_TRACK(3);
-  XLA_COUNTER("aten::{name}", 1);
-  TF_VLOG(3) << "XLA {name} :"{print_args_str};
+  LTC_FN_TRACK(3);
+  LTC_COUNTER("aten::{name}", 1);
+  LTC_VLOG(3) << "XLA {name} :"{print_args_str};
 {intermediates}
   {at_call}{collect_mutated_tensors}{update_tensors}{avoid_warning}{return_str}
 }}
diff --git a/tools/codegen/gen_backend_stubs.py b/tools/codegen/gen_backend_stubs.py
index a3b9ac254d..ce426f1b28 100644
--- a/tools/codegen/gen_backend_stubs.py
+++ b/tools/codegen/gen_backend_stubs.py
@@ -174,7 +174,7 @@ def run(source_yaml: str, output_dir: str, dry_run: bool) -> None:
                 'extra_cuda_headers': '',
                 'legacy_th_headers': '',
                 'external_backend_headers': f'''#include "{output_dir}/{backend_key}NativeFunctions.h"
-#include <torch_xla/csrc/aten_xla_type_default.h>''',
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type_default.h>''',
                 'namespaced_headers': '',
                 'DispatchKey': dispatch_key,
                 'dispatch_namespace': dispatch_key.lower(),
