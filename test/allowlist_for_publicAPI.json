{
  "torch.ao.quantization": [
    "ABC",
    "ABCMeta",
    "Any",
    "Callable",
    "Dict",
    "List",
    "Module",
    "Optional",
    "OrderedDict",
    "Pattern",
    "QConfigAny",
    "Set",
    "Tuple",
    "Type",
    "Union",
    "abstractmethod",
    "namedtuple",
    "partial",
    "type_before_parametrizations",
    "wrap_cpp_module"
  ],
  "torch.ao.quantization.fake_quantize": [
    "ABC",
    "Any",
    "FixedQParamsObserver",
    "HistogramObserver",
    "Module",
    "MovingAverageMinMaxObserver",
    "MovingAveragePerChannelMinMaxObserver",
    "Tuple",
    "abstractmethod",
    "default_fixed_qparams_range_0to1_fake_quant",
    "default_fixed_qparams_range_0to1_observer",
    "default_affine_fixed_qparams_fake_quant",
    "default_affine_fixed_qparams_observer",
    "default_dynamic_fake_quant",
    "default_embedding_fake_quant",
    "default_embedding_fake_quant_4bit",
    "default_fake_quant",
    "default_fused_act_fake_quant",
    "default_fused_per_channel_wt_fake_quant",
    "default_fused_wt_fake_quant",
    "default_histogram_fake_quant",
    "default_per_channel_weight_fake_quant",
    "default_fixed_qparams_range_neg1to1_fake_quant",
    "default_fixed_qparams_range_neg1to1_observer",
    "default_symmetric_fixed_qparams_fake_quant",
    "default_symmetric_fixed_qparams_observer",
    "default_weight_fake_quant",
    "fused_per_channel_wt_fake_quant_range_neg_127_to_127",
    "fused_wt_fake_quant_range_neg_127_to_127"
  ],
  "torch.ao.quantization.fuse_modules": [
    "List",
    "Optional",
    "fuse_conv_bn",
    "fuse_conv_bn_relu",
    "get_fuser_method",
    "type_before_parametrizations"
  ],
  "torch.ao.quantization.fuser_method_mappings": [
    "Callable",
    "Dict",
    "MatchAllNode",
    "Optional",
    "Pattern",
    "Tuple",
    "Type",
    "Union",
    "get_combined_dict"
  ],
  "torch.ao.quantization.backend_config.utils": [
    "Any",
    "Dict",
    "Callable",
    "List",
    "Union",
    "Tuple",
    "Pattern"
  ],
  "torch.ao.quantization.backend_config.native": [
    "Any",
    "Dict",
    "FixedQParamsFakeQuantize",
    "List",
    "ObservationType",
    "default_fixed_qparams_range_0to1_observer",
    "default_fixed_qparams_range_neg1to1_observer",
    "default_affine_fixed_qparams_observer",
    "default_symmetric_fixed_qparams_observer",
    "fuse_conv_bn",
    "fuse_conv_bn_relu",
    "fuse_convtranspose_bn",
    "fuse_linear_bn",
    "namedtuple",
    "reverse2",
    "reverse3",
    "reverse_sequential_wrapper2"
  ],
  "torch.ao.quantization.quantization_types": [
      "Any",
      "Node",
      "NodePattern",
      "Pattern",
      "QuantizerCls",
      "Tuple",
      "Union"
  ],
  "torch.ao.quantization.fx.graph_module": [
    "Any",
    "Dict",
    "Graph",
    "GraphModule",
    "Set",
    "Union"
  ],
  "torch.ao.quantization.fx.pattern_utils": [
    "Any",
    "Dict",
    "FixedQParamsFakeQuantize",
    "List",
    "MatchResult",
    "Node",
    "ObserverBase",
    "Optional",
    "OrderedDict",
    "Pattern",
    "QConfigAny",
    "QuantizeHandler",
    "Tuple"
  ],
  "torch.ao.quantization.fx.quantization_patterns": [
    "ABC",
    "Any",
    "Callable",
    "Dict",
    "Node",
    "NodePattern",
    "Optional",
    "Pattern",
    "all_node_args_have_no_tensors"
  ],
  "torch.ao.quantization.fx.quantization_types": [
    "Any",
    "Node",
    "NodePattern",
    "Pattern",
    "QuantizerCls",
    "Tuple",
    "Union"
  ],
  "torch.ao.quantization.fx.backend_config_utils": [
    "Any",
    "Callable",
    "DefaultFuseHandler",
    "Dict",
    "NodePattern",
    "ObservationType",
    "Optional",
    "Pattern",
    "QuantizeHandler",
    "QuantizerCls",
    "activation_dtype",
    "get_combined_dict",
    "get_default_quant_patterns",
    "get_native_backend_config_dict",
    "sorted_patterns_dict",
    "get_quantize_handler_cls"
  ],
  "torch.ao.quantization.qconfig": [
    "Any",
    "FakeQuantize",
    "FakeQuantizeBase",
    "FusedMovingAvgObsFakeQuantize",
    "HistogramObserver",
    "MovingAverageMinMaxObserver",
    "NoopObserver",
    "Optional",
    "PlaceholderObserver",
    "QConfigAny",
    "ReuseInputObserver",
    "default_debug_observer",
    "default_dynamic_fake_quant",
    "default_dynamic_quant_observer",
    "default_embedding_fake_quant",
    "default_embedding_fake_quant_4bit",
    "default_fake_quant",
    "default_float_qparams_observer",
    "default_float_qparams_observer_4bit",
    "default_fused_act_fake_quant",
    "default_fused_per_channel_wt_fake_quant",
    "default_fused_wt_fake_quant",
    "default_observer",
    "default_per_channel_weight_fake_quant",
    "default_per_channel_weight_observer",
    "default_placeholder_observer",
    "default_reuse_input_observer",
    "default_weight_fake_quant",
    "default_weight_observer",
    "fused_per_channel_wt_fake_quant_range_neg_127_to_127",
    "fused_wt_fake_quant_range_neg_127_to_127",
    "namedtuple",
    "per_channel_weight_observer_range_neg_127_to_127",
    "weight_observer_range_neg_127_to_127"
  ],
  "torch.ao.quantization.quantization_mappings": [
    "Any",
    "Callable",
    "DeQuantStub",
    "Dict",
    "Optional",
    "QuantStub",
    "Set",
    "Union",
    "default_fixed_qparams_range_0to1_fake_quant",
    "default_fixed_qparams_range_neg1to1_fake_quant",
    "default_affine_fixed_qparams_fake_quant",
    "default_symmetric_fixed_qparams_fake_quant",
    "get_combined_dict",
    "type_before_parametrizations"
  ],
  "torch.ao.quantization.quantize": [
    "DeQuantStub",
    "QuantWrapper",
    "activation_is_memoryless",
    "add_module_to_qconfig_obs_ctr",
    "get_default_dynamic_quant_module_mappings",
    "get_default_qat_module_mappings",
    "get_default_qconfig_propagation_list",
    "get_default_static_quant_module_mappings",
    "get_default_static_quant_reference_module_mappings",
    "get_qparam_dict",
    "has_no_children_ignoring_parametrizations",
    "no_observer_set",
    "type_before_parametrizations"
  ],
  "torch.ao.quantization.quantize_jit": [
    "QConfig",
    "QuantType",
    "wrap_cpp_module"
  ],
  "torch.autograd": [
    "NestedIOFunction",
    "detect_anomaly",
    "enable_grad",
    "grad",
    "gradcheck",
    "gradgradcheck",
    "inference_mode",
    "no_grad",
    "set_detect_anomaly",
    "set_grad_enabled",
    "variable"
  ],
  "torch.autograd.function": [
    "Any",
    "List",
    "Optional",
    "OrderedDict",
    "with_metaclass"
  ],
  "torch.autograd.functional": [
    "List",
    "Tuple"
  ],
  "torch.autograd.graph": [
    "Any",
    "Callable"
  ],
  "torch.autograd.profiler": [
    "Any",
    "ContextDecorator",
    "DeviceType",
    "Dict",
    "Future",
    "List",
    "Optional",
    "ProfilerActivity",
    "ProfilerConfig",
    "ProfilerState",
    "kineto_available",
    "warn"
  ],
  "torch.autograd.variable": [
    "ImperativeEngine",
    "with_metaclass"
  ],
  "torch.backends": [
    "contextmanager"
  ],
  "torch.backends.cuda": [
    "Union"
  ],
  "torch.cpu.amp.autocast_mode": [
    "Any"
  ],
  "torch.cuda": [
    "Any",
    "Device",
    "Dict",
    "List",
    "Optional",
    "Tuple",
    "Union",
    "classproperty"
  ],
  "torch.cuda.comm": [
    "broadcast",
    "broadcast_coalesced",
    "reduce_add",
    "reduce_add_coalesced",
    "scatter",
    "gather"
  ],
  "torch.cuda.amp.autocast_mode": [
    "Any"
  ],
  "torch.cuda.amp.common": [
    "find_spec"
  ],
  "torch.cuda.amp.grad_scaler": [
    "Any",
    "Dict",
    "Enum",
    "List",
    "Optional",
    "Tuple",
    "amp_definitely_not_available",
    "defaultdict"
  ],
  "torch.cuda.nccl": [
    "init_rank",
    "is_available",
    "unique_id",
    "version"
  ],
  "torch.cuda.profiler": [
    "check_error",
    "cudart"
  ],
  "torch.distributed": [
    "AllToAllOptions",
    "AllreduceCoalescedOptions",
    "AllreduceOptions",
    "BarrierOptions",
    "BroadcastOptions",
    "BuiltinCommHookType",
    "Callable",
    "DebugLevel",
    "Dict",
    "Enum",
    "FileStore",
    "GatherOptions",
    "GradBucket",
    "HashStore",
    "Logger",
    "Optional",
    "PrefixStore",
    "ProcessGroup",
    "ProcessGroupGloo",
    "ReduceOp",
    "ReduceOptions",
    "ReduceScatterOptions",
    "Reducer",
    "ScatterOptions",
    "Store",
    "TCPStore",
    "Tuple",
    "Union",
    "get_debug_level",
    "set_debug_level",
    "set_debug_level_from_env",
    "timedelta",
    "ProcessGroupMPI",
    "ProcessGroupNCCL"
  ],
  "torch.distributed.algorithms.ddp_comm_hooks.debugging_hooks": [
    "Any",
    "GradBucket"
  ],
  "torch.distributed.algorithms.ddp_comm_hooks.default_hooks": [
    "Any",
    "Callable"
  ],
  "torch.distributed.algorithms.ddp_comm_hooks.optimizer_overlap_hooks": [
    "Any",
    "Callable"
  ],
  "torch.distributed.algorithms.model_averaging.utils": [
    "Dict",
    "Iterable",
    "Iterator",
    "ProcessGroup",
    "Union",
    "group"
  ],
  "torch.distributed.autograd": [
    "DistAutogradContext",
    "backward",
    "get_gradients"
  ],
  "torch.distributed.distributed_c10d": [
    "AllToAllOptions",
    "AllreduceCoalescedOptions",
    "AllreduceOptions",
    "BarrierOptions",
    "BroadcastOptions",
    "Callable",
    "DebugLevel",
    "Dict",
    "GatherOptions",
    "Optional",
    "PrefixStore",
    "ProcessGroup",
    "ProcessGroupGloo",
    "ReduceOp",
    "ReduceOptions",
    "ReduceScatterOptions",
    "ScatterOptions",
    "Store",
    "Tuple",
    "Union",
    "get_debug_level",
    "register_rendezvous_handler",
    "rendezvous",
    "timedelta",
    "ProcessGroupMPI",
    "ProcessGroupNCCL"
  ],
  "torch.distributed.elastic.events": [
    "Dict",
    "Enum",
    "EventMetadataValue",
    "Optional"
  ],
  "torch.distributed.elastic.events.handlers": [
    "Dict"
  ],
  "torch.distributed.elastic.metrics": [
    "Optional"
  ],
  "torch.distributed.elastic.multiprocessing": [
    "Callable",
    "Dict",
    "Tuple",
    "Union",
    "get_logger"
  ],
  "torch.distributed.elastic.multiprocessing.api": [
    "Any",
    "Callable",
    "Dict",
    "FrameType",
    "IntFlag",
    "Optional",
    "ProcessFailure",
    "Set",
    "TailLog",
    "Tuple",
    "Union",
    "dataclass",
    "field",
    "nullcontext",
    "record",
    "redirect_stderr",
    "redirect_stdout"
  ],
  "torch.distributed.elastic.multiprocessing.errors": [
    "Any",
    "Callable",
    "Dict",
    "GlobalRank",
    "JSON",
    "List",
    "Optional",
    "Template",
    "Tuple",
    "TypeVar",
    "dataclass",
    "datetime",
    "field",
    "get_logger",
    "wraps"
  ],
  "torch.distributed.elastic.multiprocessing.redirects": [
    "contextmanager",
    "partial",
    "redirect_stderr",
    "redirect_stdout"
  ],
  "torch.distributed.elastic.multiprocessing.tail_log": [
    "Dict",
    "Event",
    "Future",
    "List",
    "TextIO",
    "ThreadPoolExecutor"
  ],
  "torch.distributed.elastic.rendezvous": [
    "RendezvousHandlerCreator"
  ],
  "torch.distributed.elastic.rendezvous.api": [
    "ABC",
    "Any",
    "Callable",
    "Dict",
    "Optional",
    "RendezvousHandlerCreator",
    "Store",
    "Tuple",
    "abstractmethod"
  ],
  "torch.distributed.elastic.rendezvous.dynamic_rendezvous": [
    "get_method_name"
  ],
  "torch.distributed.elastic.utils.api": [
    "Any",
    "List",
    "Template"
  ],
  "torch.distributed.elastic.utils.data.elastic_distributed_sampler": [
    "DistributedSampler"
  ],
  "torch.distributed.elastic.utils.logging": [
    "Optional",
    "get_log_level"
  ],
  "torch.distributed.elastic.utils.store": [
    "List",
    "timedelta"
  ],
  "torch.distributed.fsdp.flatten_params_wrapper": [
    "Any",
    "Dict",
    "Generator",
    "Iterator",
    "List",
    "NamedTuple",
    "Optional",
    "ParamOffset",
    "Sequence",
    "SharedParamInfo",
    "Tensor",
    "Tuple",
    "Union",
    "accumulate"
  ],
  "torch.distributed.fsdp.utils": [
    "Any",
    "Callable",
    "Dict",
    "List",
    "OrderedDict",
    "Set",
    "Tuple",
    "Union"
  ],
  "torch.distributed.fsdp.wrap": [
    "Any",
    "Callable",
    "Dict",
    "Generator",
    "Optional",
    "Set",
    "Tuple",
    "Type",
    "cast"
  ],
  "torch.distributed.nn": [
    "Function",
    "ReduceOp",
    "group"
  ],
  "torch.distributed.nn.api.remote_module": [
    "Any",
    "Callable",
    "Dict",
    "Iterator",
    "List",
    "Mapping",
    "Module",
    "Optional",
    "Parameter",
    "RemovableHandle",
    "Set",
    "Tensor",
    "Tuple",
    "Type",
    "TypeVar",
    "Union",
    "device",
    "dtype"
  ],
  "torch.distributed.nn.functional": [
    "Function",
    "ReduceOp",
    "group"
  ],
  "torch.distributed.nn.jit.instantiator": [
    "Optional",
    "get_remote_module_template"
  ],
  "torch.distributed.optim.utils": [
    "Type"
  ],
  "torch.distributed.pipeline.sync.checkpoint": [
    "Checkpoint",
    "Checkpointing",
    "Context",
    "Function",
    "Recompute",
    "ThreadLocal",
    "checkpoint",
    "enable_checkpointing",
    "enable_recomputing",
    "restore_rng_states",
    "save_rng_states"
  ],
  "torch.distributed.pipeline.sync.copy": [
    "Context",
    "Copy",
    "Wait"
  ],
  "torch.distributed.pipeline.sync.dependency": [
    "Fork",
    "Join",
    "fork",
    "join"
  ],
  "torch.distributed.pipeline.sync.microbatch": [
    "Batch",
    "NoChunk",
    "check",
    "gather",
    "scatter"
  ],
  "torch.distributed.pipeline.sync.phony": [
    "get_phony"
  ],
  "torch.distributed.pipeline.sync.pipe": [
    "BalanceError",
    "PipeSequential",
    "Pipeline",
    "WithDevice"
  ],
  "torch.distributed.pipeline.sync.pipeline": [
    "Pipeline"
  ],
  "torch.distributed.pipeline.sync.skip.layout": [
    "SkipLayout",
    "inspect_skip_layout"
  ],
  "torch.distributed.pipeline.sync.skip.portal": [
    "Context",
    "Portal",
    "PortalBlue",
    "PortalCopy",
    "PortalOrange"
  ],
  "torch.distributed.pipeline.sync.skip.skippable": [
    "Skippable"
  ],
  "torch.distributed.pipeline.sync.skip.tracker": [
    "SkipTracker",
    "SkipTrackerThroughPotals",
    "ThreadLocal",
    "current_skip_tracker",
    "use_skip_tracker"
  ],
  "torch.distributed.pipeline.sync.stream": [
    "CPUStreamType",
    "as_cuda",
    "current_stream",
    "default_stream",
    "get_device",
    "is_cuda",
    "new_stream",
    "record_stream",
    "use_device",
    "use_stream",
    "wait_stream"
  ],
  "torch.distributed.pipeline.sync.worker": [
    "Task",
    "create_workers",
    "spawn_workers",
    "worker"
  ],
  "torch.distributed.remote_device": [
    "Optional",
    "Union"
  ],
  "torch.distributed.rendezvous": [
    "Dict",
    "FileStore",
    "Iterable",
    "Optional",
    "PrefixStore",
    "Store",
    "TCPStore",
    "Tuple",
    "Union",
    "cast",
    "timedelta",
    "urlparse",
    "urlunparse"
  ],
  "torch.distributed.rpc": [
    "Any",
    "Dict",
    "Future",
    "Generator",
    "Generic",
    "GenericWithOneTypeVar",
    "PyRRef",
    "RemoteProfilerManager",
    "RpcAgent",
    "RpcBackendOptions",
    "Set",
    "Store",
    "TensorPipeAgent",
    "Tuple",
    "TypeVar",
    "WorkerInfo",
    "enable_gil_profiling",
    "get_rpc_timeout",
    "method",
    "timedelta",
    "urlparse"
  ],
  "torch.distributed.rpc.api": [
    "Any",
    "Dict",
    "Future",
    "Generic",
    "GenericWithOneTypeVar",
    "PyRRef",
    "PythonUDF",
    "RPCExecMode",
    "RemoteProfilerManager",
    "Set",
    "TypeVar",
    "WorkerInfo",
    "get_rpc_timeout",
    "method"
  ],
  "torch.distributed.rpc.backend_registry": [
    "Dict",
    "List",
    "Set",
    "Tuple"
  ],
  "torch.distributed.rpc.constants": [
    "timedelta"
  ],
  "torch.distributed.rpc.internal": [
    "Enum"
  ],
  "torch.distributed.rpc.options": [
    "DeviceType",
    "Dict",
    "List",
    "Optional",
    "Union"
  ],
  "torch.distributions.kl": [
    "Bernoulli",
    "Beta",
    "Binomial",
    "Callable",
    "Categorical",
    "Cauchy",
    "ContinuousBernoulli",
    "Dict",
    "Dirichlet",
    "Distribution",
    "Exponential",
    "ExponentialFamily",
    "Gamma",
    "Geometric",
    "Gumbel",
    "HalfNormal",
    "Independent",
    "Laplace",
    "LowRankMultivariateNormal",
    "MultivariateNormal",
    "Normal",
    "OneHotCategorical",
    "Pareto",
    "Poisson",
    "TransformedDistribution",
    "Tuple",
    "Type",
    "Uniform",
    "total_ordering"
  ],
  "torch.distributions.utils": [
    "Any",
    "Dict",
    "Number",
    "is_tensor_like",
    "update_wrapper"
  ],
  "torch.functional": [
    "istft",
    "pca_lowrank",
    "svd_lowrank"
  ],
  "torch.futures": [
    "Future"
  ],
  "torch.fx": [
    "ProxyableClassMeta",
    "Tracer",
    "symbolic_trace",
    "wrap"
  ],
  "torch.fx.experimental.unification.core": [
    "Iterator",
    "assoc",
    "dispatch",
    "isvar",
    "partial",
    "unify",
    "walk"
  ],
  "torch.fx.experimental.unification.dispatch": [
    "dispatch",
    "partial"
  ],
  "torch.fx.experimental.unification.more": [
    "dispatch",
    "reify",
    "unify"
  ],
  "torch.fx.experimental.unification.multipledispatch.conflict": [
    "groupby",
    "isvariadic"
  ],
  "torch.fx.experimental.unification.multipledispatch.core": [
    "Dispatcher",
    "MethodDispatcher"
  ],
  "torch.fx.experimental.unification.multipledispatch.dispatcher": [
    "AmbiguityWarning",
    "Variadic",
    "ambiguities",
    "expand_tuples",
    "isvariadic",
    "ordering",
    "super_signature",
    "warn"
  ],
  "torch.fx.experimental.unification.multipledispatch.utils": [
    "OrderedDict"
  ],
  "torch.fx.experimental.unification.multipledispatch.variadic": [
    "typename"
  ],
  "torch.fx.experimental.unification.unification_tools": [
    "first",
    "getter",
    "groupby"
  ],
  "torch.fx.experimental.unification.variable": [
    "contextmanager",
    "dispatch",
    "hashable",
    "isvar"
  ],
  "torch.fx.graph": [
    "Any",
    "Argument",
    "Callable",
    "Dict",
    "FrozenSet",
    "List",
    "NamedTuple",
    "Node",
    "Optional",
    "Set",
    "Target",
    "TransformCodeFunc",
    "Tuple",
    "Type",
    "compatibility",
    "contextmanager",
    "dataclass",
    "map_arg"
  ],
  "torch.fx.graph_module": [
    "Any",
    "Dict",
    "Graph",
    "Importer",
    "List",
    "Optional",
    "PackageExporter",
    "PackageImporter",
    "Path",
    "PythonCode",
    "Set",
    "Type",
    "Union",
    "compatibility"
  ],
  "torch.fx.immutable_collections": [
    "Any",
    "Context",
    "Dict",
    "List",
    "Tuple",
    "compatibility"
  ],
  "torch.fx.operator_schemas": [
    "Any",
    "Callable",
    "Dict",
    "List",
    "NamedTuple",
    "OpOverload",
    "OpOverloadPacket",
    "Optional",
    "Tuple",
    "cast",
    "compatibility"
  ],
  "torch.fx.passes.graph_drawer": [
    "Any",
    "Dict",
    "TensorMetadata",
    "chain",
    "compatibility"
  ],
  "torch.fx.proxy": [
    "assert_fn"
  ],
  "torch.hub": [
    "HTTPError",
    "Path",
    "Request",
    "tqdm",
    "urlopen",
    "urlparse"
  ],
  "torch.jit": [
    "Attribute",
    "Final",
    "Iterator",
    "ONNXTracedModule",
    "RecursiveScriptClass",
    "RecursiveScriptModule",
    "ScriptModule",
    "ScriptWarning",
    "TopLevelTracedModule",
    "TracedModule",
    "TracerWarning",
    "TracingCheckError",
    "contextmanager",
    "export",
    "fork",
    "freeze",
    "fuser",
    "ignore",
    "interface",
    "is_scripting",
    "is_tracing",
    "jit_module_from_flatbuffer",
    "last_executed_optimized_graph",
    "load",
    "optimize_for_inference",
    "optimized_execution",
    "run_frozen_optimizations",
    "save",
    "save_jit_module_to_flatbuffer",
    "script",
    "script_method",
    "set_fusion_strategy",
    "set_module",
    "trace",
    "trace_module",
    "unused",
    "wait"
  ],
  "torch.jit.annotations": [
    "Any",
    "AnyType",
    "ComplexType",
    "Dict",
    "DictType",
    "EvalEnv",
    "FloatType",
    "IntType",
    "List",
    "ListType",
    "StringType",
    "TensorType",
    "Tuple",
    "TupleType",
    "get_enum_value_type",
    "is_dict",
    "is_function_or_method",
    "is_list",
    "is_optional",
    "is_tensor",
    "is_tuple",
    "is_union",
    "is_vararg"
  ],
  "torch.linalg": [
    "LinAlgError",
    "Tensor"
  ],
  "torch.multiprocessing": [
    "Array",
    "AuthenticationError",
    "Barrier",
    "BoundedSemaphore",
    "BufferTooShort",
    "Condition",
    "Event",
    "JoinableQueue",
    "Lock",
    "Manager",
    "Pipe",
    "Pool",
    "Process",
    "ProcessContext",
    "ProcessError",
    "ProcessExitedException",
    "ProcessRaisedException",
    "Queue",
    "RLock",
    "RawArray",
    "RawValue",
    "Semaphore",
    "SimpleQueue",
    "SpawnContext",
    "TimeoutError",
    "Value",
    "active_children",
    "allow_connection_pickling",
    "cpu_count",
    "current_process",
    "freeze_support",
    "get_all_start_methods",
    "get_context",
    "get_logger",
    "get_start_method",
    "init_reductions",
    "log_to_stderr",
    "set_executable",
    "set_forkserver_preload",
    "set_start_method",
    "spawn",
    "start_processes",
    "parent_process"
  ],
  "torch.nn.functional": [
    "Tensor"
  ],
  "torch.nn.init": [
    "Tensor"
  ],
  "torch.nn.intrinsic.modules": [
    "_FusedModule"
  ],
  "torch.nn.intrinsic.qat.modules.linear_fused": [
    "Parameter",
    "fuse_linear_bn_weights"
  ],
  "torch.nn.intrinsic.quantized.modules.conv_relu": [
    "fuse_conv_bn_weights"
  ],
  "torch.nn.modules.linear": [
    "NonDynamicallyQuantizableLinear"
  ],
  "torch.nn.modules.rnn": [
    "apply_permutation"
  ],
  "torch.nn.parallel": [
    "DistributedDataParallelCPU"
  ],
  "torch.nn.parallel.comm": [
    "List"
  ],
  "torch.nn.parallel.parallel_apply": [
    "ExceptionWrapper",
    "autocast"
  ],
  "torch.nn.parallel.replicate": [
    "OrderedDict"
  ],
  "torch.nn.parallel.scatter_gather": [
    "is_namedtuple"
  ],
  "torch.nn.parameter": [
    "OrderedDict"
  ],
  "torch.nn.qat.dynamic.modules.linear": [
    "activation_is_memoryless"
  ],
  "torch.nn.quantized": [
    "MaxPool2d"
  ],
  "torch.nn.quantized.dynamic.modules.rnn": [
    "apply_permutation"
  ],
  "torch.nn.quantized.functional": [
    "List",
    "Optional",
    "Tensor"
  ],
  "torch.nn.quantized.modules": [
    "MaxPool2d",
    "_ConvNd"
  ],
  "torch.nn.quantized.modules.batchnorm": [
    "Tensor"
  ],
  "torch.nn.quantized.modules.utils": [
    "repeat"
  ],
  "torch.nn.utils.rnn": [
    "bind",
    "PackedSequence_"
  ],
  "torch.nn.utils.convert_parameters": [
    "Iterable",
    "Optional"
  ],
  "torch.onnx": [
    "Dict",
    "OperatorExportTypes",
    "Optional",
    "TensorProtoDataType",
    "TrainingMode"
  ],
  "torch.overrides": [
    "BaseTorchFunctionMode",
    "TorchFunctionMode",
    "TorchFunctionModeMeta",
    "enable_torch_function_mode",
    "get_default_nowrap_functions",
    "has_torch_function",
    "push_torch_function_mode"
  ],
  "torch.package.analyze.is_from_package": [
    "Any",
    "ModuleType",
    "is_mangled"
  ],
  "torch.package.find_file_dependencies": [
    "List",
    "Optional",
    "Tuple"
  ],
  "torch.package.glob_group": [
    "GlobPattern",
    "Iterable",
    "Union"
  ],
  "torch.profiler": [
    "DeviceType",
    "ProfilerActivity",
    "kineto_available",
    "record_function"
  ],
  "torch.quantization": [
    "ABC",
    "DeQuantStub",
    "FakeQuantize",
    "FakeQuantizeBase",
    "FixedQParamsFakeQuantize",
    "FusedMovingAvgObsFakeQuantize",
    "HistogramObserver",
    "MinMaxObserver",
    "MovingAverageMinMaxObserver",
    "MovingAveragePerChannelMinMaxObserver",
    "NoopObserver",
    "ObserverBase",
    "PerChannelMinMaxObserver",
    "PlaceholderObserver",
    "QConfig",
    "QConfigAny",
    "QConfigDynamic",
    "QuantStub",
    "QuantType",
    "QuantWrapper",
    "RecordingObserver",
    "add_module_to_qconfig_obs_ctr",
    "add_observer_",
    "add_quant_dequant",
    "assert_valid_qconfig",
    "convert",
    "convert_dynamic_jit",
    "convert_jit",
    "default_fixed_qparams_range_0to1_fake_quant",
    "default_affine_fixed_qparams_fake_quant",
    "default_debug_observer",
    "default_dynamic_quant_observer",
    "default_fake_quant",
    "default_float_qparams_observer",
    "default_fused_act_fake_quant",
    "default_fused_per_channel_wt_fake_quant",
    "default_fused_wt_fake_quant",
    "default_histogram_fake_quant",
    "default_histogram_observer",
    "default_observer",
    "default_per_channel_weight_fake_quant",
    "default_per_channel_weight_observer",
    "default_placeholder_observer",
    "default_fixed_qparams_range_neg1to1_fake_quant",
    "default_symmetric_fixed_qparams_fake_quant",
    "default_weight_fake_quant",
    "default_weight_observer",
    "disable_fake_quant",
    "disable_observer",
    "enable_fake_quant",
    "enable_observer",
    "fuse_conv_bn",
    "fuse_conv_bn_jit",
    "fuse_conv_bn_relu",
    "fuse_linear_bn",
    "fuse_modules",
    "get_default_compare_output_module_list",
    "get_default_dynamic_quant_module_mappings",
    "get_default_float_to_quantized_operator_mappings",
    "get_default_qat_module_mappings",
    "get_default_qat_qconfig",
    "get_default_qconfig",
    "get_default_qconfig_propagation_list",
    "get_default_static_quant_module_mappings",
    "get_dynamic_quant_module_class",
    "get_fuser_method",
    "get_observer_dict",
    "get_observer_state_dict",
    "get_quantized_operator",
    "get_static_quant_module_class",
    "get_unique_devices_",
    "is_activation_post_process",
    "load_observer_state_dict",
    "no_observer_set",
    "prepare",
    "prepare_dynamic_jit",
    "prepare_jit",
    "prepare_qat",
    "propagate_qconfig_",
    "qconfig_equals",
    "quant_type_to_str",
    "quantize",
    "quantize_dynamic",
    "quantize_dynamic_jit",
    "quantize_jit",
    "quantize_qat",
    "register_activation_post_process_hook",
    "script_qconfig",
    "script_qconfig_dict",
    "swap_module"
  ],
  "torch.quantization.fake_quantize": [
    "FakeQuantize",
    "FakeQuantizeBase",
    "FixedQParamsFakeQuantize",
    "FusedMovingAvgObsFakeQuantize",
    "default_fixed_qparams_range_0to1_fake_quant",
    "default_affine_fixed_qparams_fake_quant",
    "default_fake_quant",
    "default_fused_act_fake_quant",
    "default_fused_per_channel_wt_fake_quant",
    "default_fused_wt_fake_quant",
    "default_histogram_fake_quant",
    "default_per_channel_weight_fake_quant",
    "default_fixed_qparams_range_neg1to1_fake_quant",
    "default_symmetric_fixed_qparams_fake_quant",
    "default_weight_fake_quant",
    "disable_fake_quant",
    "disable_observer",
    "enable_fake_quant",
    "enable_observer"
  ],
  "torch.quantization.fuse_modules": [
    "fuse_conv_bn",
    "fuse_conv_bn_relu",
    "fuse_known_modules",
    "fuse_modules",
    "get_fuser_method"
  ],
  "torch.quantization.fuser_method_mappings": [
    "fuse_conv_bn",
    "fuse_conv_bn_relu",
    "fuse_linear_bn",
    "get_fuser_method"
  ],
  "torch.quantization.observer": [
    "ABC",
    "HistogramObserver",
    "MinMaxObserver",
    "MovingAverageMinMaxObserver",
    "MovingAveragePerChannelMinMaxObserver",
    "NoopObserver",
    "ObserverBase",
    "PerChannelMinMaxObserver",
    "PlaceholderObserver",
    "RecordingObserver",
    "default_debug_observer",
    "default_dynamic_quant_observer",
    "default_float_qparams_observer",
    "default_histogram_observer",
    "default_observer",
    "default_per_channel_weight_observer",
    "default_placeholder_observer",
    "default_weight_observer",
    "get_observer_state_dict",
    "load_observer_state_dict"
  ],
  "torch.quantization.qconfig": [
    "QConfig",
    "QConfigAny",
    "QConfigDynamic",
    "add_module_to_qconfig_obs_ctr",
    "assert_valid_qconfig",
    "get_default_qat_qconfig",
    "get_default_qconfig",
    "qconfig_equals"
  ],
  "torch.quantization.quant_type": [
    "QuantType",
    "quant_type_to_str"
  ],
  "torch.quantization.quantization_mappings": [
    "get_default_compare_output_module_list",
    "get_default_dynamic_quant_module_mappings",
    "get_default_float_to_quantized_operator_mappings",
    "get_default_qat_module_mappings",
    "get_default_qconfig_propagation_list",
    "get_default_static_quant_module_mappings",
    "get_dynamic_quant_module_class",
    "get_quantized_operator",
    "get_static_quant_module_class",
    "no_observer_set"
  ],
  "torch.quantization.quantize": [
    "add_observer_",
    "add_quant_dequant",
    "convert",
    "get_observer_dict",
    "get_unique_devices_",
    "is_activation_post_process",
    "prepare",
    "prepare_qat",
    "propagate_qconfig_",
    "quantize",
    "quantize_dynamic",
    "quantize_qat",
    "register_activation_post_process_hook",
    "swap_module"
  ],
  "torch.quantization.quantize_jit": [
    "convert_dynamic_jit",
    "convert_jit",
    "fuse_conv_bn_jit",
    "prepare_dynamic_jit",
    "prepare_jit",
    "quantize_dynamic_jit",
    "quantize_jit",
    "script_qconfig",
    "script_qconfig_dict"
  ],
  "torch.quantization.stubs": [
    "DeQuantStub",
    "QuantStub",
    "QuantWrapper"
  ],
  "torch.quasirandom": [
    "Optional"
  ],
  "torch.random": [
    "Generator"
  ],
  "torch.return_types": [
    "attr",
    "pytree_register_structseq"
  ],
  "torch.serialization": [
    "Any",
    "BinaryIO",
    "Dict",
    "IO",
    "Optional",
    "Storage",
    "Tuple",
    "Type",
    "Union",
    "cast",
    "closing",
    "contextmanager",
    "get_source_lines_and_file"
  ],
  "torch.sparse": [
    "BFloat16Tensor",
    "ByteTensor",
    "CharTensor",
    "DoubleTensor",
    "FloatTensor",
    "HalfTensor",
    "IntTensor",
    "LongTensor",
    "ShortTensor",
    "addmm",
    "log_softmax",
    "mm",
    "softmax"
  ],
  "torch.storage": [
    "Any",
    "Storage",
    "Type",
    "TypeVar",
    "Union",
    "cast",
    "lru_cache"
  ],
  "torch.testing": [
    "FileCheck",
    "all_types",
    "all_types_and",
    "all_types_and_complex",
    "all_types_and_complex_and",
    "all_types_and_half",
    "assert_allclose",
    "assert_close",
    "complex_types",
    "double_types",
    "empty_types",
    "floating_and_complex_types",
    "floating_and_complex_types_and",
    "floating_types",
    "floating_types_and",
    "floating_types_and_half",
    "get_all_complex_dtypes",
    "get_all_device_types",
    "get_all_dtypes",
    "get_all_fp_dtypes",
    "get_all_int_dtypes",
    "get_all_math_dtypes",
    "integral_types",
    "integral_types_and",
    "make_non_contiguous",
    "make_tensor",
    "rand",
    "randn"
  ],
  "torch.types": [
    "Any",
    "Device",
    "List",
    "Number",
    "Sequence",
    "Tuple",
    "Union"
  ],
  "torch.utils": [
    "disable_minidumps",
    "enable_minidumps",
    "enable_minidumps_on_exceptions"
  ],
  "torch.utils.benchmark.utils.compare": [
    "Colorize",
    "Table",
    "optional_min"
  ],
  "torch.utils.benchmark.utils.cpp_jit": [
    "Any",
    "CallgrindModuleType",
    "List",
    "Optional",
    "TimeitModuleType"
  ],
  "torch.utils.benchmark.utils.fuzzer": [
    "dtype_size",
    "prod"
  ],
  "torch.utils.benchmark.utils.sparse_fuzzer": [
    "FuzzedTensor",
    "Number",
    "Optional",
    "Tuple",
    "Union"
  ],
  "torch.utils.benchmark.utils.timer": [
    "CPPTimer",
    "timer"
  ],
  "torch.utils.benchmark.utils.valgrind_wrapper.timer_interface": [
    "GlobalsBridge",
    "Serialization",
    "wrapper_singleton"
  ],
  "torch.utils.cpp_extension": [
    "ExtensionVersioner",
    "FileBaton",
    "GeneratedFileCleaner",
    "List",
    "Optional",
    "TorchVersion",
    "Tuple",
    "Union",
    "build_ext",
    "get_hip_file_path"
  ],
  "torch.utils.data": [
    "_DatasetKind",
    "argument_validation",
    "default_collate",
    "default_convert",
    "functional_datapipe",
    "get_worker_info",
    "guaranteed_datapipes_determinism",
    "non_deterministic",
    "runtime_validation",
    "runtime_validation_disabled"
  ],
  "torch.utils.data.dataloader": [
    "default_collate",
    "default_convert",
    "get_worker_info"
  ],
  "torch.utils.data.datapipes.dataframe": [
    "DFIterDataPipe"
  ],
  "torch.utils.dlpack": [
    "Any",
    "to_dlpack"
  ],
  "torch.utils.hipify.hipify_python": [
    "Dict",
    "HipifyFinalResult",
    "HipifyResult",
    "Iterable",
    "Iterator",
    "List",
    "Mapping",
    "Optional"
  ],
  "torch.utils.hooks": [
    "Any",
    "OrderedDict"
  ],
  "torch.utils.show_pickle": [
    "Any",
    "BinaryIO",
    "IO",
    "Union"
  ],
  "torch": [
    "BFloat16Storage",
    "BFloat16Tensor",
    "ComplexDoubleStorage",
    "ComplexFloatStorage",
    "DisableTorchFunction",
    "Generator",
    "HalfStorage",
    "HalfTensor",
    "QInt32Storage",
    "QInt8Storage",
    "QUInt2x4Storage",
    "QUInt4x2Storage",
    "QUInt8Storage",
    "Storage",
    "autocast",
    "broadcast_shapes",
    "candidate",
    "compiled_with_cxx11_abi",
    "from_dlpack",
    "lobpcg",
    "lu",
    "obj",
    "set_default_dtype",
    "set_grad_enabled",
    "set_printoptions",
    "unique"
  ]
}
