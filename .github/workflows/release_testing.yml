name: PyTorch release testing CI (pytorch-linux-py3.8-cu102)
on:
  pull_request:
  workflow_dispatch:
    inputs:
      pytorch_version:
        description: "The version of PyTorch to test"
        required: true
        default: "1.11.0"

jobs:
  release-testing:
    runs-on: [self-hosted, bm-runner]
    env:
      GPU_FREQUENCY: "5001,900"
      CUDA_VISIBLE_DEVICES: "0"
      CORE_LIST: "24-47"
      GOMP_CPU_AFFINITY: "24-47"
      CUDA_VERSION: "11.3"
      MAGMA_VERSION: "magma-cuda113"
    steps:
      - name: Check out pytorch/pytorch
        uses: actions/checkout@v2
        with:
          path: pytorch
      - name: Check out pytorch/examples
        uses: actions/checkout@v2
        with:
          repository: pytorch/examples
          path: examples
      - name: Checkout pytorch/benchmark
        uses: actions/checkout@v2
        with:
          repository: pytorch/benchmark
          path: benchmark
      - name: Create Conda env
        run: |
          conda create -y -n reltest python=3.8
      - name: Install PyTorch and setup environment
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          # re-setup the cuda soft link
          if [ -e "/usr/local/cuda" ]; then
              sudo rm /usr/local/cuda
          fi
          sudo ln -sf /usr/local/cuda-${CUDA_VERSION} /usr/local/cuda
          conda install -y cudatoolkit=${CUDA_VERSION}
          conda install -y -c pytorch ${MAGMA_VERSION}
          # link: https://anaconda.org/pytorch/pytorch
          # conda install -y -c pytorch pytorch=1.11.0 torchvision torchtext
          # echo "RESULT_DIR=${HOME}/release-testing/1.11.0" >> $GITHUB_ENV
          # link: https://anaconda.org/pytorch-test/pytorch
          # conda install -y -c pytorch-test pytorch=1.12.0 torchvision torchtext
          # pip install https://download.pytorch.org/whl/nightly/cu113/torch-1.12.0.dev20220512%2Bcu113-cp38-cp38-linux_x86_64.whl \
          # 	      https://download.pytorch.org/whl/nightly/cu113/torchvision-0.13.0.dev20220512%2Bcu113-cp38-cp38-linux_x86_64.whl \
          #             https://download.pytorch.org/whl/nightly/torchtext-0.13.0.dev20220512-cp38-cp38-linux_x86_64.whl
          # echo "RESULT_DIR=${HOME}/release-testing/20220512" >> $GITHUB_ENV
          # pip install https://download.pytorch.org/whl/nightly/cu113/torch-1.12.0.dev20220424%2Bcu113-cp38-cp38-linux_x86_64.whl \
          #             https://download.pytorch.org/whl/nightly/cu113/torchvision-0.13.0.dev20220424%2Bcu113-cp38-cp38-linux_x86_64.whl \
          #             https://download.pytorch.org/whl/nightly/torchtext-0.13.0.dev20220424-cp38-cp38-linux_x86_64.whl
          # pip install https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp38-cp38-linux_x86_64.whl \
          #             https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp38-cp38-linux_x86_64.whl \
          #             https://download.pytorch.org/whl/torchtext-0.12.0-cp38-cp38-linux_x86_64.whl
          # install build deps
          NUMPY_VERSION="1.21.2"
          conda install -y numpy="${NUMPY_VERSION}" requests ninja pyyaml mkl mkl-include \
                           setuptools cmake=3.22 cffi typing_extensions \
                           future six dataclasses pillow pytest tabulate gitpython git-lfs tqdm psutil
          rm -rf pytorch-src || true
          git clone https://github.com/pytorch/pytorch.git pytorch-src
          pushd pytorch-src
          # commit on 20220401
          # git checkout c0a6add7eef8ab3715c0f4281b3b7a2a0f1e24b5
          # commit on 20220501
          git checkout 201ddafc22e22c387b4cd654f397e05354d73d09
          git submodule update --recursive --init
          # git checkout release/1.11
          git setup.py clean
          python setup.py install
          echo "RESULT_DIR=${HOME}/release-testing/20220410-src" >> $GITHUB_ENV
          popd; python -c "import torch; print(torch.__version__); print(torch.version.git_version)"
          # install build deps
          sudo nvidia-smi -ac ${GPU_FREQUENCY}
          echo "Result dir: ${RESULT_DIR} "
      - name: Check machine tuned
        run: |
          echo "Make sure the machine is tuned."
          . activate reltest
          pushd benchmark
          pip install -U py-cpuinfo psutil distro
          sudo $HOME/anaconda3/envs/reltest/bin/python3 torchbenchmark/util/machine_config.py
          popd
      - name: Run MNist
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          mkdir -p ${RESULT_DIR}/mnist
          pushd examples/mnist
          export LOG_FILE=${RESULT_DIR}/mnist/result.log
          export MEM_FILE=${RESULT_DIR}/mnist/result_mem.log
          # taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 1
      - name: Run MNist HogWild
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          mkdir -p ${RESULT_DIR}/mnist_hogwild
          pushd examples/mnist_hogwild
          export LOG_FILE=${RESULT_DIR}/mnist_hogwild/result.log
          export MEM_FILE=${RESULT_DIR}/mnist_hogwild/result_mem.log
          # taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 10
      - name: Run CPU WLM LSTM
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          mkdir -p ${RESULT_DIR}/wlm_cpu_lstm
          pushd examples/word_language_model
          export LOG_FILE=${RESULT_DIR}/wlm_cpu_lstm/result.log
          export MEM_FILE=${RESULT_DIR}/wlm_cpu_lstm/result_mem.log
          # taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 10 --model LSTM
      - name: Run GPU WLM LSTM
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          mkdir -p ${RESULT_DIR}/wlm_gpu_lstm
          pushd examples/word_language_model
          export LOG_FILE=${RESULT_DIR}/wlm_gpu_lstm/result.log
          export MEM_FILE=${RESULT_DIR}/wlm_gpu_lstm/result_mem.log
          # taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 10 --model LSTM --cuda
      - name: Run CPU WLM Transformer
        run: |
          set -x
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          python -c 'import torch'
          mkdir -p ${RESULT_DIR}/wlm_cpu_trans
          pushd examples/word_language_model
          export LOG_FILE=${RESULT_DIR}/wlm_cpu_trans/result.log
          export MEM_FILE=${RESULT_DIR}/wlm_cpu_trans/result_mem.log
          taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 10 --model Transformer
      - name: Run GPU WLM Transformer
        run: |
          . activate reltest
          . switch-cuda.sh ${CUDA_VERSION}
          nvcc --version
          mkdir -p ${RESULT_DIR}/wlm_gpu_trans
          pushd examples/word_language_model
          export LOG_FILE=${RESULT_DIR}/wlm_gpu_trans/result.log
          export MEM_FILE=${RESULT_DIR}/wlm_gpu_trans/result_mem.log
          # taskset -c $CORE_LIST bash ../../pytorch/.github/scripts/monitor_proc.sh python main.py --epochs 10 --model Transformer --cuda
      - name: Remove Conda env
        run: |
          conda env remove --name reltest
      - name: Submit GH artifacats
        uses: actions/upload-artifact@v2
        with:
          name: Benchmark result
          path: ${{ env.RESULT_DIR }}
